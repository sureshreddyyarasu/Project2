{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e3f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark import StorageLevel\n",
    "df=spark.read.csv(\"dbfs:/FileStore/shared_uploads/shalininunavath19241@gmail.com/bank_transactions.csv\",inferSchema=True,header=True)\n",
    "from pyspark import StorageLevel\n",
    "df1=spark.read.csv(\"dbfs:/FileStore/shared_uploads/shalininunavath19241@gmail.com/bank_transactions.csv\",inferSchema=True,header=True)\n",
    "df2=df1.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "#print(type(df2))\n",
    "def run_query():\n",
    "    while True:\n",
    "        print(''' \n",
    "                1.Number of customers with balance > 500 and transaction amount < 50000: \n",
    "                2. Average account balance of female customers: \n",
    "                3. Female customers located in Chennai: \n",
    "                4. Group customers by gender and show the total transaction amount for each group \n",
    "                5. The highest transaction amount for each day in Mumbai \n",
    "                6. The average transaction amount for all transactions \n",
    "                7. The highest transaction amount \n",
    "                8. The lowest transaction amount \n",
    "                9. The customer with the highest transaction amount \n",
    "                10. T1ransactions made on a specific date (e.g. \"21-10-2016\") \n",
    "                11. Balance of each customer on a specific date range (e.g. \"03-08-2016\" to \"21-10-2016\") \n",
    "                12. The customers with the highest account balance \n",
    "                13. Group transactions by location and the total transaction amount and count for each group \n",
    "                14. Balance of each customer during a specific time range (e.g. \"52256\" to \"183144\") \n",
    "                15. Maximum customer balance for each location \n",
    "                16.The top 10 customers with the highest TransactionAmount\n",
    "                17. Find the top 10 customers with the highest TransactionAmount  \n",
    "                18. Exit ''')\n",
    "        query_number= int(input(\"Enter your choice: \"))\n",
    "        if query_number == 1:\n",
    "            df.filter((df[\"CustAccountBalance\"]>500) & (df[\"TransactionAmount (INR)\"]<50000)).show()\n",
    "        elif query_number == 2:\n",
    "            df.filter(df[\"CustGender\"] == \"F\").selectExpr(\"avg(CustAccountBalance)\").show() \n",
    "        elif query_number == 3:\n",
    "            df.filter((df[\"CustGender\"]==\"F\") & (df[\"CustLocation\"]==\"CHENNAI\")).show() \n",
    "        elif query_number == 4:\n",
    "            df.select(\"TransactionAmount (INR)\").agg({\"TransactionAmount (INR)\": \"avg\"}).show()\n",
    "        elif query_number == 5:\n",
    "            df.select(\"TransactionAmount (INR)\").agg({\"TransactionAmount (INR)\": \"max\"}).show() \n",
    "        elif query_number == 6:\n",
    "            df.select(\"TransactionAmount (INR)\").agg({\"TransactionAmount (INR)\": \"min\"}).show()\n",
    "        elif query_number == 7:\n",
    "            from pyspark.sql.functions import desc\n",
    "            #import pandas as p\n",
    "            sorted_transactions = df.sort(desc(\"TransactionTime\"))\n",
    "            highest_transaction = sorted_transactions.first()\n",
    "            print(\"TransactionID:\",highest_transaction.TransactionTime)\n",
    "            print(\"Transaction Time:\",highest_transaction.TransactionTime)\n",
    "        elif query_number == 8:\n",
    "            result = df.filter(df['CustGender'] == \"M\")\n",
    "            result.persist(StorageLevel.DISK_ONLY)\n",
    "            result.show()   \n",
    "        elif query_number == 9:\n",
    "            from pyspark.sql.functions import col\n",
    "            df.filter(col(\"TransactionDate\") == \"2016-08-06\").show()\n",
    "        elif query_number == 10:\n",
    "            from pyspark.sql.functions import col, sum\n",
    "            filtered_transactions_df = df.filter(col(\"TransactionDate\").between(\"03-08-2016\", \"21-10-2016\"))\n",
    "            filtered_transactions_df.show()\n",
    "        elif query_number == 11:\n",
    "            df.orderBy(\"CustAccountBalance\", ascending=False).show()\n",
    "        elif query_number == 12:\n",
    "            df.groupBy(\"CustLocation\").agg({\"TransactionAmount (INR)\": \"sum\", \"TransactionID\": \"count\"}).show()\n",
    "        elif query_number == 13:\n",
    "            from pyspark.sql.functions import col, sum\n",
    "            filtered_transactions_df = df.filter(col(\"TransactionTime\").between(\"52256\", \"183144\"))\n",
    "            filtered_transactions_df.show()\n",
    "            customer_balances_df = filtered_transactions_df.groupBy(\"CustomerID\")\\\n",
    "                .agg(sum(\"TransactionAmount (INR)\").alias(\"TotalTransactionAmount\"))\\\n",
    "                .join(df.select(\"CustomerID\", \"CustAccountBalance\"), on=\"CustomerID\", how=\"left\")\\\n",
    "                .withColumn(\"CustomerBalance\", col(\"CustAccountBalance\") + col(\"TotalTransactionAmount\"))\n",
    "            customer_balances_df.show()   \n",
    "        elif query_number == 14:\n",
    "            from pyspark.sql.functions import desc\n",
    "            top_10_customers = df.groupBy(\"CustomerID\").agg({\"TransactionID\": \"sum\"}) \\\n",
    "                                .orderBy(desc(\"sum(TransactionID)\"))\n",
    "            print(top_10_customers.head())\n",
    "        elif query_number==15:\n",
    "            from pyspark.sql.functions import col, sum\n",
    "\n",
    "            # Filter transactions by date range\n",
    "            filtered_transactions_df = df.filter(col(\"TransactionDate\").between(\"2016-08-01\", \"2016-08-06\"))\n",
    "\n",
    "            # Print the filtered DataFrame\n",
    "            filtered_transactions_df.show()\n",
    "        elif query_number==16:\n",
    "            #17 maximum customer balancr for each location.\n",
    "            df.createOrReplaceTempView(\"transactions\")\n",
    "            spark.sql(\"SELECT t1.CustomerID, t1.CustLocation, t1.CustAccountBalance FROM transactions t1 INNER JOIN (SELECT CustLocation, MAX(CustAccountBalance) AS MaxBalance FROM transactions GROUP BY CustLocation) t2 ON t1.CustLocation = t2.CustLocation AND t1.CustAccountBalance = t2.MaxBalance\").show()\n",
    "        elif query_number==17:\n",
    "\n",
    "            #Find the top 10 customers with the highest TransactionAmount\n",
    "            from pyspark.sql.functions import desc\n",
    "            top_10_customers1 = df.orderBy(desc(\"TransactionAmount (INR)\")).limit(10)\n",
    "            top_10_customers1.show()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"Invalid choice\")  \n",
    "run_query()\n",
    "\n",
    "\n",
    "username = \"Team_titans\"\n",
    "pswd = \"Titans@21\"\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
